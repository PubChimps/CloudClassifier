{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 667,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import sys\n",
    "import re\n",
    "from collections import *\n",
    "import operator\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parsenb(dataset):\n",
    "    parseddataset = []\n",
    "   \n",
    "    for i in dataset:\n",
    "        code = ''\n",
    "        try:\n",
    "            parsednb = (json.loads(i[1]))\n",
    "        except:\n",
    "            print(sys.exc_info())\n",
    "        for j in range(len(parsednb['cells'])):\n",
    "            if parsednb['cells'][j]['cell_type'] == 'code':\n",
    "                code = code + ''.join(parsednb['cells'][j]['source'])\n",
    "        parseddataset.append([code, i[2]])\n",
    "    \n",
    "    for i in range(len(parseddataset)):\n",
    "        parseddataset[i][0] = re.sub('[^a-zA-Z0-9 \\n\\.]', ' ', parseddataset[i][0]).replace('\\n', ' ')\n",
    "    return parseddataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 731,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayes_train(pldict, samples):\n",
    "    plprobs = {}\n",
    "    counts = Counter()\n",
    "    for i in pldict.keys():\n",
    "        plprobs[i] = float(len(pldict[i]))/samples\n",
    "        \n",
    "    plwordprobs = {}\n",
    "    plwordcounts = {}\n",
    "    for pl in pldict.keys():\n",
    "        plwordprobs[pl] = {}\n",
    "        plwordcounts[pl] = 0\n",
    "    \n",
    "    for pl in pldict.keys():\n",
    "        for i in pldict[pl]:\n",
    "            counts.update(filter(None, re.split(r'[^\\w]', re.sub(re.compile(\"/\\*.*?\\*/\",re.DOTALL ) ,\"\" ,i))))\n",
    "            for word in counts.keys():\n",
    "                if word not in plwordprobs[pl]:\n",
    "                    plwordprobs[pl][word] = counts[word]\n",
    "                else:\n",
    "                    plwordprobs[pl][word] += counts[word]\n",
    "                plwordcounts[pl] += counts[word]\n",
    "            plwordcount = 0\n",
    "            counts = Counter()\n",
    "    for pl in plwordprobs.keys():   \n",
    "        for word in plwordprobs[pl]:\n",
    "            plwordprobs[pl][word] = float(plwordprobs[pl][word])/plwordcounts[pl]\n",
    "        \n",
    "    return plprobs, plwordprobs\n",
    "    \n",
    "def bayes_test(testdata,plprob,plwordprob):\n",
    "    Ypred = []\n",
    "\n",
    "    for row in testdata:\n",
    "        testcounter = Counter()\n",
    "        testcounter.update(filter(None, re.split(r'[^\\w]', re.sub(re.compile(\"/\\*.*?\\*/\",re.DOTALL ) ,\"\" ,str(row[0])))))\n",
    "\n",
    "        prob = {}\n",
    "        for key in plprob.keys():\n",
    "            prob[key] = 0\n",
    "        for key in prob.keys():\n",
    "            for i in testcounter:\n",
    "                if i not in plwordprobs[key]:\n",
    "                    plwordprob[key][i] = 1e-4\n",
    "                else:\n",
    "                    plwordprob[key][i] += 1e-4\n",
    "                prob[key] += testcounter[i]*np.log(plwordprob[key][i])\n",
    "            prob[key] += np.log(plprob[key])\n",
    "        Ypred.append(max(prob.items(), key=operator.itemgetter(1))[0])\n",
    "    \n",
    "    return Ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters_deep(layer_dims):\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)            # number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l - 1]) * 0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "\n",
    "        \n",
    "    return parameters\n",
    "\n",
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "def forward_propagation(X, parameters):\n",
    "    A = X\n",
    "\n",
    "    cache = {}\n",
    "    for i in range(1,len(params)//2):\n",
    "        Aprev = A\n",
    "        Z = np.dot(parameters['W' + str(i)], Aprev) + parameters['b' + str(i)]\n",
    "        A = np.tanh(Z)\n",
    "        cache['Z' + str(i)] = Z\n",
    "        cache['A' + str(i)] = A\n",
    "        \n",
    "    ZL = np.dot(parameters['W' + str(len(params)//2)], A) + parameters['b' + str(len(params)//2)]\n",
    "    AL = softmax(ZL)\n",
    "    cache['Z' + str(len(params)//2)] = ZL\n",
    "    cache['A' + str(len(params)//2)] = AL\n",
    "    \n",
    "    return AL, cache\n",
    "\n",
    "def cross_entropy(A,Y):\n",
    "    m = Y.shape[1]\n",
    "    logprobs = np.multiply(np.log(A), Y) + np.multiply((1 - Y), np.log(1 - A))\n",
    "    cost = - np.sum(logprobs) / m \n",
    "    \n",
    "    return np.squeeze(cost)\n",
    "\n",
    "def backward_propagation(parameters, cache, X, Y):\n",
    "    \n",
    "    grads = {}\n",
    "    \n",
    "    cache['A0'] = X\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    dZ = cache['A' + str(len(params)//2)] - Y\n",
    "    dWL = (1 / m) * np.dot(dZ, cache['A' + str(len(params)//2 - 1)].T)\n",
    "    dbL = (1 / m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "    grads['dW' + str(len(params)//2)] = dWL\n",
    "    grads['db' + str(len(params)//2)] = dbL\n",
    "    \n",
    "    for i in reversed(range(len(params)//2)[1::]):\n",
    "        dZprev = dZ\n",
    "        dZ = np.multiply(np.dot(parameters['W' + str(i+1)].T, dZprev), 1 - np.power(cache['A' + str(i)], 2))\n",
    "        dW = (1 / m) * np.dot(dZ, cache['A' + str(i-1)].T)\n",
    "        db = (1 / m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "        \n",
    "        grads['dW' + str(i)] = dW\n",
    "        grads['db' + str(i)] = db\n",
    "        \n",
    "    return grads\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l + 1)] = parameters[\"W\" + str(l + 1)] - learning_rate * grads[\"dW\" + str(l + 1)]\n",
    "        parameters[\"b\" + str(l + 1)] = parameters[\"b\" + str(l + 1)] - learning_rate * grads[\"db\" + str(l + 1)]\n",
    "      \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 745,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(pred, testdata):\n",
    "    count = 0\n",
    "    for i in range(len(pred)):\n",
    "        if pred[i]==testdata[i][1]:\n",
    "            count += 1\n",
    "    return float(count)/len(pred)\n",
    "\n",
    "\n",
    "def entropy(predlist):\n",
    "    ent = 0\n",
    "    if predlist.count('aws') != 0:\n",
    "        ent -= predlist.count('aws')/len(predlist)*np.log2(predlist.count('aws')/len(predlist))\n",
    "    if predlist.count('ibm') != 0:\n",
    "        ent -= predlist.count('ibm')/len(predlist)*np.log2(predlist.count('ibm')/len(predlist))\n",
    "    if predlist.count('ms') != 0:\n",
    "        ent -= predlist.count('ms')/len(predlist)*np.log2(predlist.count('ms')/len(predlist))\n",
    "    return ent\n",
    "\n",
    "def caticc(lista, listb, listc):\n",
    "    diff = []\n",
    "    for i in range(len(lista)):\n",
    "        if lista[i] == listb[i] == listc[i]:\n",
    "            diff.append(0)\n",
    "        elif lista[i] == listb[i] or listb[i] == listc[i] or lista[i] == listc[i]:\n",
    "            diff.append(1)\n",
    "        else:\n",
    "            diff.append(4)\n",
    "    \n",
    "    return np.sum(diff)/(np.sum(diff) + entropy(lista) + entropy(listb) + entropy(listc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<class 'json.decoder.JSONDecodeError'>, JSONDecodeError('Expecting value: line 2 column 1 (char 1)',), <traceback object at 0x1a3cd9c188>)\n"
     ]
    }
   ],
   "source": [
    "dataset2 = np.load('cloudata.npy')\n",
    "dataset2 = parsenb(dataset2)\n",
    "np.random.shuffle(dataset2)\n",
    "trainset2, testset2 = np.array(dataset2)[:int(len(dataset2)*.8),:], np.array(dataset2)[int(len(dataset2)*.8):,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 738,
   "metadata": {},
   "outputs": [],
   "source": [
    "words2 = []\n",
    "\n",
    "labeledlines2 = []\n",
    "ignore_words = ['?', ',']\n",
    "\n",
    "for line in trainset2:\n",
    "    text = str(re.split(r'[.,]', line[0])).replace(\"'\",\"\").replace('[','')\n",
    "    text = re.sub(r'\\b\\w{1,1}\\b', '', text)\n",
    "    w = nltk.word_tokenize(text)\n",
    "    w = [ele for ele in w if ele not in ignore_words]\n",
    "    words2.extend(w)\n",
    "    labeledlines2.append([w, line[1]])\n",
    "    \n",
    "testlines2 = []\n",
    "\n",
    "for line in testset2:\n",
    "    text = str(re.split(r'[.,]', line[0])).replace(\"'\",\"\").replace('[','')\n",
    "    text = re.sub(r'\\b\\w{1,1}\\b', '', text)\n",
    "    w = nltk.word_tokenize(text)\n",
    "    w = [ele for ele in w if ele not in ignore_words]\n",
    "    words2.extend(w)\n",
    "    testlines2.append([w, line[1]])\n",
    "    \n",
    "\n",
    "words2 = list(set(words2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = []\n",
    "\n",
    "for line in labeledlines2:\n",
    "    bag = []\n",
    "    code = line[0]\n",
    "    for w in words2:\n",
    "        bag.append(code.count(w)) \n",
    "        \n",
    "    classes = [0,0,0]\n",
    "    if line[1] == 'aws':\n",
    "        classes[0] = 1\n",
    "    elif line[1] == 'watson':\n",
    "        classes[1] = 1\n",
    "    elif line[1] == 'azure':\n",
    "        classes[2] = 1\n",
    "\n",
    "    data2.append([bag,classes])\n",
    "    \n",
    "testdata2 = []\n",
    "for line in testlines2:\n",
    "    bag = []\n",
    "    code = line[0]\n",
    "    for w in words2:\n",
    "        bag.append(code.count(w)) \n",
    "        \n",
    "    classes = [0,0,0]\n",
    "    if line[1] == 'aws':\n",
    "        classes[0] = 1\n",
    "    elif line[1] == 'watson':\n",
    "        classes[1] = 1\n",
    "    elif line[1] == 'azure':\n",
    "        classes[2] = 1\n",
    "\n",
    "    testdata2.append([bag,classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array([row[0] for row in data2]).T\n",
    "y_train = np.array([row[1] for row in data2]).T\n",
    "x_test = np.array([row[0] for row in testdata2]).T\n",
    "y_test = np.array([row[1] for row in testdata2]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 681,
   "metadata": {},
   "outputs": [],
   "source": [
    "pls = {}\n",
    "for row in range(len(trainset2)):\n",
    "    if trainset2[row][1] not in pls:\n",
    "        pls[trainset2[row][1]] = []\n",
    "    pls[trainset2[row][1]].append(trainset2[row][0])\n",
    "\n",
    "plprobs, plwordprobs = bayes_train(pls, len(trainset2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'aws': 0.6862068965517242,\n",
       " 'azure': 0.15689655172413794,\n",
       " 'watson': 0.15689655172413794}"
      ]
     },
     "execution_count": 696,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plrobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tf', 0.027677463818056514),\n",
       " ('df', 0.02263266712611992),\n",
       " ('train', 0.01386629910406616),\n",
       " ('data', 0.012598208132322536),\n",
       " ('import', 0.011936595451412818),\n",
       " ('test', 0.011853893866299104),\n",
       " ('print', 0.01155065472088215),\n",
       " ('in', 0.009317711922811854),\n",
       " ('for', 0.009235010337698139),\n",
       " ('the', 0.008766368022053756),\n",
       " ('model', 0.008656099241902136),\n",
       " ('cost', 0.008435561681598897),\n",
       " ('sess', 0.00647829083390765),\n",
       " ('to', 0.00647829083390765),\n",
       " ('from', 0.006340454858718126),\n",
       " ('shape', 0.005403170227429359),\n",
       " ('run', 0.005375603032391455),\n",
       " ('as', 0.005265334252239835),\n",
       " ('get', 0.005155065472088215),\n",
       " ('batch', 0.004824259131633356),\n",
       " ('if', 0.004796691936595451),\n",
       " ('and', 0.004769124741557547),\n",
       " ('self', 0.0047139903514817364),\n",
       " ('values', 0.004631288766368022),\n",
       " ('random', 0.004383184011026878)]"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(plwordprobs['watson'].items(), key=operator.itemgetter(1) ,reverse=True)[:25]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 742,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayespred = bayes_test(plprobs,plwordprobs,testset2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 1.909213\n",
      "Cost after iteration 100: 0.488878\n",
      "Cost after iteration 200: 0.210986\n",
      "Cost after iteration 300: 0.123227\n",
      "Cost after iteration 400: 0.102383\n",
      "Cost after iteration 500: 0.091373\n",
      "Cost after iteration 600: 0.083705\n",
      "Cost after iteration 700: 0.077960\n",
      "Cost after iteration 800: 0.073648\n",
      "Cost after iteration 900: 0.070461\n",
      "Cost after iteration 1000: 0.068141\n",
      "Cost after iteration 2000: 0.062132\n",
      "Cost after iteration 3000: 0.061410\n",
      "Cost after iteration 4000: 0.061163\n",
      "Cost after iteration 5000: 0.061042\n",
      "Cost after iteration 6000: 0.060970\n",
      "Cost after iteration 7000: 0.060924\n",
      "Cost after iteration 8000: 0.060891\n",
      "Cost after iteration 9000: 0.060866\n"
     ]
    }
   ],
   "source": [
    "params = initialize_parameters_deep([len(words2),1024,512,64,3])\n",
    "\n",
    "for i in range(0, 10000):\n",
    "    AL, cache = forward_propagation_L(x_train, params)\n",
    "    cost = cross_entropy(AL, y_train)\n",
    "    grads = backward_propagation_L(params, cache, x_train, y_train)\n",
    "    params = update_parameters(params, grads, .1)\n",
    "    \n",
    "    if cost and i % 100 == 0 and i < 1000 or i % 1000 == 0:\n",
    "        print (\"Cost after iteration %i: %f\" % (i, cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AL, cache = forward_propagation_L(x_train, params)\n",
    "nnpred = []\n",
    "\n",
    "for i in AL.T:\n",
    "    if np.argmax(i) == 0:\n",
    "        nnpred.append('aws')\n",
    "    if np.argmax(i) == 1:\n",
    "        nnpred.append('watson')\n",
    "    if np.argmax(i) == 2:\n",
    "        nnpred.append('azure')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "from watson_developer_cloud import NaturalLanguageClassifierV1\n",
    "\n",
    "natural_language_classifier = NaturalLanguageClassifierV1(\n",
    "    username=\"YOUR USERNAME\",\n",
    "    password=\"YOUR PASSWORD\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = []\n",
    "for i in labeledlines2:\n",
    "    for j in re.findall('.{1,1024}', str(i[0]).replace(',', ' ').replace(\"'\",'').replace('[','').replace(']','')):\n",
    "        d.append({'text': j, 'cloud': i[1]})\n",
    "        \n",
    "df = pd.DataFrame(d, columns = ['text', 'cloud'])\n",
    "df['text'].replace(' ', np.nan, inplace=True)\n",
    "df = df.dropna()\n",
    "df.to_csv('newcloudtraindata.csv', header=['text','cloud'],index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('newcloudtraindata.csv', 'rb') as training_data:\n",
    "    natural_language_classifier.create_classifier(training_data=training_data, metadata='{\"name\": \"new Cloud Classifier\",\"language\": \"en\"}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "natural_language_classifier.list_classifiers() #get your classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier_id': 'f33041x451-nlc-2892',\n",
       " 'created': '2018-10-15T23:09:57.475Z',\n",
       " 'language': 'en',\n",
       " 'name': 'new Cloud Classifier',\n",
       " 'status': 'Available',\n",
       " 'status_description': 'The classifier instance is now available and is ready to take classifier requests.',\n",
       " 'url': 'https://gateway.watsonplatform.net/natural-language-classifier/api/v1/classifiers/f33041x451-nlc-2892'}"
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "natural_language_classifier.get_classifier('YOUR CLASSIFIER ID').get_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [],
   "source": [
    "watsonpred = []\n",
    "for i in testset2:\n",
    "    x = natural_language_classifier.classify(,re.sub(' +',' ',\" \".join(re.split(r'[^\\w]', re.sub(re.compile(\"/\\*.*?\\*/\",re.DOTALL ) ,\"\" ,i[0]))))[0:1024])\n",
    "    watsonpred.append(x.get_result()['top_class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 747,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8344827586206897\n",
      "0.8896551724137931\n",
      "0.9448275862068966\n"
     ]
    }
   ],
   "source": [
    "print(compute_accuracy(bayespred,testset2))\n",
    "print(compute_accuracy(nnpred,testset2))\n",
    "print(compute_accuracy(watsonpred, testset2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9643151120180458"
      ]
     },
     "execution_count": 627,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caticc(nnpred,predictions,watsonpred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
